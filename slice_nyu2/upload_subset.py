#!/bin/env python
"""
This script uploads the NYU2 dataset subset generated by create_subset.py
to S3.

Config (global vars)
* source: Source (NYUv2 subset) directory path
* dest: Target S3 bucket and prefix (multiple objects will be inserted)

Inserted Objects:
* prefix: [{
	"id": <id (not necessarily equal to scene id)>
}]
* prefixid: {
	"id": <scene id (might be different from id)>
	"rgb": <base64 encoded RGB-24bit .png>
	"depth": <base64 encoded gray-16bit .png>
}

All RGB-D images are assumed to be generated by kinect1.
"""
from __future__ import division, print_function
import base64
import boto
import boto.s3.connection
import boto.s3.key
import cv
import cv2
import json
import os

source = '/data-new/research/2014/reconstruction/NYU2-slice'
dest = {
	"bucket": "recon3d",
	"prefix": "nyu2-slice-"
}

credential = {
	"key": "AKIAJ3IWENP74M7MZQCQ",
	"secret": "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",
}


count = {
	"scene": 0,
	"scene_valid": 0,
}

# collect good scenes and create whole thing on memory
all_scenes = []
for scene_dir in os.listdir(source):
	count["scene"] += 1
	scene_dir_abs = os.path.join(source, scene_dir)
	files = os.listdir(scene_dir_abs)
	if len(files) != 2:
		continue
	[rgb_file] = [f for f in files if f.endswith('.ppm')]
	[depth_file] = [f for f in files if f.endswith('.pgm')]
	print('Reading %s' % scene_dir)
	rgb = cv2.imread(os.path.join(scene_dir_abs, rgb_file), cv.CV_LOAD_IMAGE_COLOR)
	depth = cv2.imread(os.path.join(scene_dir_abs, depth_file), cv.CV_LOAD_IMAGE_GRAYSCALE | 2)  # CV_LOAD_IMAGE_ANYDEPTH == 2
	assert(rgb.shape == (480, 640, 3))
	assert(depth.shape == (480, 640))

	json_data = {
		"id": scene_dir,
		"rgb": base64.b64encode(cv2.imencode('.png', rgb)[1]),
		"depth": base64.b64encode(cv2.imencode('.png', depth)[1]),
	}
	all_scenes.append({
		"id": scene_dir,
		"json_data": json.dumps(json_data),
		"object_id": dest["prefix"] + scene_dir,
	})
	count["scene_valid"] += 1

conn = boto.s3.connection.S3Connection(credential["key"], credential["secret"])
bucket = conn.get_bucket(dest["bucket"])

# upload each scene
for scene in all_scenes:
	key = boto.s3.key.Key(bucket)
	key.key = scene["object_id"]
	print("Uploading to %s" % key.key)
	key.set_contents_from_string(scene["json_data"])

# extract index
index = []
for scene in all_scenes:
	index.append({
		"id": scene["id"],
		"object_id": scene["object_id"]
		})
key = boto.s3.key.Key(bucket)
key.key = dest["prefix"]
print("Uploading to %s" % key.key)
key.set_contents_from_string(json.dumps(index))

# show stats
print(count)
